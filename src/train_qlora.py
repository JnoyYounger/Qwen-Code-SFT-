import os
import torch
import torch.distributed as dist # <--- å¯¼å…¥åˆ†å¸ƒå¼åº“
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import json
import argparse

# âœ… 1. å‘½ä»¤è¡Œå‚æ•°è§£æ
parser = argparse.ArgumentParser()
parser.add_argument("--model_path", type=str, default="./Qwen1.5-1.8B-Chat", help="Path to base model directory")
parser.add_argument("--data_path", type=str, default="data/Opencode.jsonl", help="Path to processed dataset")
parser.add_argument("--output_dir", type=str, default="outputs_2500", help="Path to save the fine-tuned model")
parser.add_argument("--max_seq_length", type=int, default=1024, help="Maximum sequence length for tokenization")
parser.add_argument("--max_steps", type=int, default=1000, help="Total number of training steps")
parser.add_argument("--batch_size", type=int, default=2, help="Batch size per device (Set to 1 for <11GB GPUs)")
parser.add_argument("--grad_accum", type=int, default=8, help="Gradient accumulation steps")
parser.add_argument("--num_proc", type=int, default=16, help="Number of CPU cores for tokenization")
args = parser.parse_args()


# -------------------------------
# åŠ è½½æ•°æ®
# -------------------------------
print(f"âœ… Loading dataset from {args.data_path}")
dataset = load_dataset(
    "json", 
    data_files=args.data_path,
    cache_dir="./hf_cache"  # ç£ç›˜ç©ºé—´ä¿®å¤
)
train_data = dataset["train"]

# -------------------------------
# åŠ è½½ tokenizer
# -------------------------------
print(f"âœ… Loading tokenizer from {args.model_path}")
tokenizer = AutoTokenizer.from_pretrained(
    args.model_path,
    local_files_only=True
)
tokenizer.pad_token = tokenizer.eos_token


# <-- é‡å†™ tokenize_function ä»¥æ”¯æŒæ‰¹é‡ (batched=True)
def tokenize_function(examples): # 'example' -> 'examples'
    texts = [
        prompt + "\n" + response 
        for prompt, response in zip(examples["prompt"], examples["response"])
    ]
    # æ·»åŠ  eos token (Qwen1.5-Chat æ ¼å¼æ¨è)
    texts = [t + tokenizer.eos_token for t in texts]
    
    return tokenizer(
        texts,
        truncation=True,
        padding=False, 
        max_length=args.max_seq_length,
    )

# -------------------------------
# ğŸ‘‡ å…³é”®ä¿®å¤: åŒæ­¥æ•°æ®å¤„ç†
# -------------------------------
# æˆ‘ä»¬å¿…é¡»ä» os.environ ä¸­æ˜¾å¼è¯»å– "LOCAL_RANK"
# `accelerate launch` ä¼šä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½®è¿™ä¸ªå˜é‡ (0, 1, 2, 3...)
local_rank = int(os.environ.get("LOCAL_RANK", "0"))
# æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦å¤„äº DDP (åˆ†å¸ƒå¼) æ¨¡å¼
is_ddp = int(os.environ.get("WORLD_SIZE", "1")) > 1

# å¦‚æœæ˜¯ DDP å¹¶ä¸” PyTorch åˆ†å¸ƒå¼è¿˜æ²¡åˆå§‹åŒ–ï¼Œåˆ™åˆå§‹åŒ–
# (Accelerate å¯åŠ¨æ—¶é€šå¸¸å·²ç»ä¸ºæˆ‘ä»¬åšäº†ï¼Œä½†è¿™æ˜¯ä¸ªå®‰å…¨ä¿éšœ)
if is_ddp and not dist.is_initialized():
    dist.init_process_group(backend='nccl')

print(f"âœ… Tokenizing dataset with max_length={args.max_seq_length} using {args.num_proc} cores... [Rank {local_rank}]")

if is_ddp and local_rank != 0:
    # -------------------------------
    # ğŸ‘‡ éä¸»è¿›ç¨‹åœ¨æ­¤ç­‰å¾…
    # -------------------------------
    print(f"[Rank {local_rank}] Waiting for main process (rank 0) to tokenize data...")
    dist.barrier() # ç­‰å¾… rank 0 å®Œæˆ

tokenized_datasets = train_data.map(
    tokenize_function, 
    batched=True,      # å¼€å¯æ‰¹é‡å¤„ç†
    num_proc=args.num_proc,  # å¯ç”¨å¤šæ ¸å¤„ç†
    remove_columns=train_data.column_names
)

if is_ddp and local_rank == 0:
    # -------------------------------
    # ğŸ‘‡ Rank 0 (ä¸»è¿›ç¨‹) å®Œæˆåï¼Œé€šçŸ¥å…¶ä»–è¿›ç¨‹
    # -------------------------------
    print(f"[Rank 0] Tokenization complete. Signaling other processes...")
    dist.barrier() # é€šçŸ¥å…¶ä»–è¿›ç¨‹ "ç¼“å­˜å·²å‡†å¤‡å¥½"

if is_ddp:
     print(f"[Rank {local_rank}] Proceeding after tokenization barrier.")
# -------------------------------
# ğŸ‘† ä¿®å¤ç»“æŸ
# -------------------------------


# -------------------------------
# åŠ è½½ Qwen æ¨¡å‹ï¼ˆQLoRA é…ç½®ï¼‰
# -------------------------------
print("âœ… Loading base model with 4-bit quantization (QLoRA)...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

print(f"Loading model on local_rank: {local_rank} (handled by Accelerate)...")
model = AutoModelForCausalLM.from_pretrained(
    args.model_path,
    quantization_config=bnb_config,
    # -------------------------------
    # ğŸ‘‡ å…³é”®ä¿®å¤:
    # ä½¿ç”¨ local_rank æ•´æ•°ï¼Œè€Œä¸æ˜¯ torch.cuda.current_device()
    # -------------------------------
    device_map={'': local_rank}, 
    low_cpu_mem_usage=True,
    local_files_only=True,
    torch_dtype=torch.float16 # <--- ä¿®å¤åŠ è½½ OOM
)

# -------------------------------
# å…³é”®ä¿®å¤ï¼šä¸º k-bit è®­ç»ƒ + æ¢¯åº¦æ£€æŸ¥ç‚¹å‡†å¤‡æ¨¡å‹
# -------------------------------
model = prepare_model_for_kbit_training(model) # ä¿®å¤ 'no grad_fn' é”™è¯¯


# -------------------------------
# è®¾ç½® LoRA é…ç½®
# -------------------------------
print("âœ… Setting up optimized LoRA config...")
lora_config = LoraConfig(
    r=32,                  
    lora_alpha=64,           
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "up_proj", "gate_proj", "down_proj"
    ],                      
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# -------------------------------
# è®­ç»ƒå‚æ•°è®¾ç½®
# -------------------------------
training_args = TrainingArguments(
    output_dir=args.output_dir,
    per_device_train_batch_size=args.batch_size,    
    gradient_accumulation_steps=args.grad_accum,    
    learning_rate=2e-4,
    
    max_steps=args.max_steps,        
    #num_train_epochs=1,     # <-- æ·»åŠ è¿™ä¸€è¡Œ (æˆ–è€… 2, 3)
    fp16=True,
    gradient_checkpointing=True,    # èŠ‚çœæ˜¾å­˜
    
    # <-- âœ… MODIFIED: æ·»åŠ è¿™ä¸€è¡Œæ¥ä¿®å¤ DDP + GC å†²çª
    gradient_checkpointing_kwargs={'use_reentrant': False},
    
    save_total_limit=3,
    logging_steps=10,
    save_steps=200,
    eval_strategy="no",
    
    lr_scheduler_type="cosine",    
    warmup_steps=100,              
    
    report_to="none"
)

# -------------------------------
# Data Collator (åŠ¨æ€ Padding)
# -------------------------------
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# -------------------------------
# å¼€å§‹è®­ç»ƒ
# -------------------------------
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    tokenizer=tokenizer,
    data_collator=data_collator
)

print("ğŸš€ Starting QLoRA fine-tuning...")
trainer.train()
print("âœ… Training complete! Saving model...")
trainer.save_model(args.output_dir)
print(f"âœ… Model saved to {args.output_dir}")