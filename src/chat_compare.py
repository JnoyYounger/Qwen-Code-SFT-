import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# -----------------------------------------------------------------
# ğŸ‘‡ ä¿®å¤é‡ç‚¹ï¼šchat å‡½æ•°å¢åŠ äº† apply_chat_template å’Œåˆ‡ç‰‡è§£ç 
# -----------------------------------------------------------------
def chat(model, tokenizer, prompt):
    """
    ä½¿ç”¨æ¨¡å‹è¿›è¡Œå¯¹è¯ç”Ÿæˆ (ä¿®å¤ç‰ˆ)ã€‚
    """
    # 1. æ„å»ºç¬¦åˆ Qwen æ ¼å¼çš„å¯¹è¯æ¶ˆæ¯
    messages = [
        {"role": "user", "content": prompt}
    ]
    
    # 2. åº”ç”¨å¯¹è¯æ¨¡æ¿ (Chat Template)
    # è¿™ä¼šè‡ªåŠ¨æ·»åŠ  <|im_start|>user...<|im_end|><|im_start|>assistant ç­‰ç‰¹æ®Šæ ‡è®°
    # add_generation_prompt=True ä¼šå‘Šè¯‰æ¨¡å‹"ç°åœ¨è½®åˆ°ä½ è¯´è¯äº†"
    text = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # 3. è½¬ä¸º Tensor å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,  # ğŸ‘ˆ å»ºè®®æ”¹å°ä¸€ç‚¹ï¼Œé˜²æ­¢åºŸè¯å¤ªå¤šï¼Œé€šå¸¸ 512 å¤Ÿç”¨äº†
            do_sample=True,
            temperature=0.7,     # æ¸©åº¦ï¼Œå¤ªé«˜å®¹æ˜“èƒ¡è¯ï¼Œå¤ªä½å®¹æ˜“æ­»æ¿
            top_p=0.9,
            repetition_penalty=1.1, # é‡å¤æƒ©ç½š
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id # é˜²æ­¢æŠ¥è­¦
        )

    # 4. å…³é”®æ­¥éª¤ï¼šåªè§£ç æ–°ç”Ÿæˆçš„ tokenï¼Œå»æ‰è¾“å…¥çš„ prompt éƒ¨åˆ†
    # model.generate è¿”å›çš„æ˜¯ [è¾“å…¥+è¾“å‡º]ï¼Œæˆ‘ä»¬åªéœ€è¦ [è¾“å‡º]
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response.strip()
# -----------------------------------------------------------------


def main():
    base_path = "./Qwen1.5-1.8B-Chat"
    adapter_path = "./outputs_10000/checkpoint-10000" # æˆ–è€…ç›´æ¥ ./outputs

    print(f"ğŸ”¹ åŠ è½½åŸºç¡€æ¨¡å‹ {base_path}")
    base_tokenizer = AutoTokenizer.from_pretrained(
        base_path, 
        local_files_only=True,
        trust_remote_code=True # Qwen æœ‰æ—¶éœ€è¦è¿™ä¸ª
    )
    base_model = AutoModelForCausalLM.from_pretrained(
        base_path,
        device_map="auto",
        torch_dtype=torch.float16,
        local_files_only=True,
        trust_remote_code=True
    )

    print(f"ğŸ”¹ åŠ è½½å¾®è°ƒæ¨¡å‹ {adapter_path}")
    # æ³¨æ„ï¼šå¾®è°ƒæ¨¡å‹çš„ Tokenizer é€šå¸¸å’ŒåŸºåº§ä¸€æ ·ï¼Œé™¤éä½ æ”¹äº†è¯è¡¨ï¼Œå¦åˆ™å¯ä»¥ç›´æ¥å¤ç”¨ base_tokenizer
    # è¿™é‡Œä¸ºäº†ä¿é™©èµ·è§è¿˜æ˜¯åŠ è½½ä¸€éï¼Œä½†è¦æ³¨æ„ adapter è·¯å¾„é‡Œæœ‰æ²¡æœ‰ tokenizer.json
    try:
        ft_tokenizer = AutoTokenizer.from_pretrained(adapter_path, local_files_only=True)
    except:
        print("âš ï¸ Adapter è·¯å¾„æœªæ‰¾åˆ° tokenizerï¼Œå¤ç”¨åŸºåº§ tokenizer")
        ft_tokenizer = base_tokenizer

    # (A) åŠ è½½ç”¨äºå¾®è°ƒæ¼”ç¤ºçš„åŸºåº§æ¨¡å‹ (ä¸ºäº†å¯¹æ¯”ï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹)
    # æ˜¾å­˜å¦‚æœä¸å¤Ÿï¼Œè¿™ä¸€æ­¥ä¼šæŠ¥é”™ã€‚å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå»ºè®®æ¯æ¬¡åªåŠ è½½ä¸€ä¸ªæ¨¡å‹æµ‹è¯•ã€‚
    print("... æ­£åœ¨åŠ è½½å¾®è°ƒæ¨¡å‹çš„åŸºåº§...")
    ft_base_model = AutoModelForCausalLM.from_pretrained(
        base_path,
        device_map="auto",
        torch_dtype=torch.float16,
        local_files_only=True,
        trust_remote_code=True
    )

    # (B) åº”ç”¨ LoRA
    print("... æ­£åœ¨åº”ç”¨ LoRA adapter ...")
    ft_model = PeftModel.from_pretrained(ft_base_model, adapter_path, local_files_only=True)

    print("\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹å®æ—¶å¯¹è¯æµ‹è¯•ï¼")
    print("è¾“å…¥ 'exit' é€€å‡ºã€‚")

    while True:
        prompt = input("\nğŸ§© è¯·è¾“å…¥æµ‹è¯•å†…å®¹ï¼š")
        if prompt.strip().lower() in ["exit", "quit"]:
            print("ğŸ‘‹ å·²é€€å‡ºã€‚")
            break

        print("\n--- ğŸ§  åŸºç¡€æ¨¡å‹å›ç­” ---")
        # åŸºç¡€æ¨¡å‹ä¹Ÿè¦ç”¨ apply_chat_templateï¼Œå¦åˆ™æ•ˆæœä¹Ÿä¼šå˜å·®
        base_ans = chat(base_model, base_tokenizer, prompt)
        print(base_ans)

        print("\n--- ğŸš€ å¾®è°ƒæ¨¡å‹å›ç­” ---")
        ft_ans = chat(ft_model, ft_tokenizer, prompt)
        print(ft_ans)
        print("\n" + "="*60)

if __name__ == "__main__":
    main()